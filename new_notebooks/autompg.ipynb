{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T17:55:19.109012Z",
     "start_time": "2025-03-13T17:55:18.626652Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpyro\n",
    "\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:55:19.118536Z",
     "start_time": "2025-03-13T17:55:19.109370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GREEN_RGB_COLORS = [\n",
    "    '#004c00',  # '#004e00', '#005000', '#005100', '#005300',\n",
    "    # '#005500', # '#005700', '#005900', '#005a00', '#005c00',\n",
    "    '#005e00',  # '#006000', '#006200', '#006300', '#006500',\n",
    "    # '#006700', # '#006900', '#006b00', '#006c00', '#006e00',\n",
    "    '#007000',  # '#007200', '#007400', '#007500', '#007700',\n",
    "    # '#007900', # '#007b00', '#007d00', '#007e00', '#008000',\n",
    "    '#008200',  # '#008400', '#008600', '#008800', '#008900',\n",
    "    # '#008b00', # '#008d00', '#008f00', '#009100', '#009200',\n",
    "    '#009400',  # '#009600', '#009800', '#009a00', '#009b00',\n",
    "    # '#009d00', # '#009f00', '#00a100', '#00a300', '#00a400',\n",
    "    '#00a600',  # '#00a800', '#00aa00', '#00ac00', '#00ad00',\n",
    "    # '#00af00', # '#00b100', '#00b300', '#00b500', '#00b600',\n",
    "    '#00b800',  # '#00ba00', '#00bc00', '#00be00', '#00bf00',\n",
    "    # '#00c100', # '#00c300', '#00c500', '#00c700', '#00c800',\n",
    "    '#00ca00',  # '#00cc00', '#00ce00', '#00d000', '#00d100',\n",
    "    # '#00d300', # '#00d500', '#00d700', '#00d900', '#00da00',\n",
    "    '#00dc00',  # '#00de00', '#00e000', '#00e200', '#00e300',\n",
    "    # '#00e500', # '#00e700', '#00e900', '#00eb00', '#00ec00',\n",
    "    '#00ee00',  # '#00f000', '#00f200', '#00f400', '#00f500',\n",
    "    # '#00f700', # '#00f900', '#00fb00', '#00fd00', '#00ff00'\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "a3c7f87bb6b32b2e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:55:22.112624Z",
     "start_time": "2025-03-13T17:55:22.075995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_chains = 10\n",
    "n_devices = min(os.cpu_count(), num_chains)\n",
    "os.environ['XLA_FLAGS'] = f'--xla_force_host_platform_device_count={n_devices}'\n",
    "\n",
    "import jax\n",
    "\n",
    "print(f\"Default backend for JAX: {jax.default_backend()}\")\n",
    "print(\n",
    "    f\"Number of devices available on default backend: \"\n",
    "    f\"{jax.local_device_count(backend=jax.default_backend())}\"\n",
    ")"
   ],
   "id": "a1193a097d9dd473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default backend for JAX: cpu\n",
      "Number of devices available on default backend: 10\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:55:27.650579Z",
     "start_time": "2025-03-13T17:55:23.297195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from namgcv.basemodels.bnam import BayesianNAM\n",
    "from namgcv.configs.experimental.autompg_nam import DefaultBayesianNAMConfig\n",
    "from namgcv.configs.experimental.autompg_nn import DefaultBayesianNNConfig"
   ],
   "id": "82921725f1f7c35e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:55:27,442 - datasets - INFO - PyTorch version 2.6.0 available.\n",
      "2025-03-13 18:55:27,449 - datasets - INFO - JAX version 0.4.28 available.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:39:49.565498Z",
     "start_time": "2025-03-13T17:39:49.512361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    \"autompg\",\n",
    "    as_frame=True,\n",
    "    return_X_y=True,\n",
    "    version=1\n",
    ")\n",
    "\n",
    "# One hot encode the categorical features.\n",
    "cat_features = pd.get_dummies(\n",
    "    data=X[[col for col in X.columns if X[col].dtype == \"category\"]]\n",
    ").astype(int)\n",
    "num_features = X[[col for col in X.columns if X[col].dtype != \"category\"]]\n",
    "print(f\"Categorical features: {cat_features.columns.tolist()}\")\n",
    "print(f\"Numeric features: {num_features.columns.tolist()}\")"
   ],
   "id": "d1bf7feaeaa1db67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['cylinders_3', 'cylinders_4', 'cylinders_5', 'cylinders_6', 'cylinders_8', 'model_70', 'model_71', 'model_72', 'model_73', 'model_74', 'model_75', 'model_76', 'model_77', 'model_78', 'model_79', 'model_80', 'model_81', 'model_82', 'origin_1', 'origin_2', 'origin_3']\n",
      "Numeric features: ['displacement', 'horsepower', 'weight', 'acceleration']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:54:40.999457Z",
     "start_time": "2025-03-13T17:39:52.247113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_outputs = 1  # Mean regression only.\n",
    "\n",
    "cat_feature_info, cat_feature_inputs = {}, {}\n",
    "for cat_col in [col for col in X.columns if X[col].dtype == \"category\"]:\n",
    "    all_cols_for_feature = [\n",
    "        col for col in cat_features.columns if col.startswith(cat_col)\n",
    "    ]\n",
    "    cat_feature_info[cat_col] = {\n",
    "        \"input_dim\": len(all_cols_for_feature),\n",
    "        \"output_dim\": num_outputs\n",
    "    }\n",
    "    cat_feature_inputs[cat_col] = jnp.array(\n",
    "        cat_features[all_cols_for_feature]\n",
    "    )\n",
    "\n",
    "num_feature_info = {\n",
    "    feature_name: {\n",
    "        \"input_dim\": 1,\n",
    "        \"output_dim\": num_outputs\n",
    "    } for feature_name in num_features.columns\n",
    "}\n",
    "num_feature_inputs = {\n",
    "    feature_name: jnp.array(\n",
    "        num_features.loc[:, feature_name]\n",
    "    ) for feature_name in num_feature_info.keys()\n",
    "}\n",
    "\n",
    "numpyro.set_host_device_count(\n",
    "    DefaultBayesianNAMConfig().num_chains\n",
    ")\n",
    "model = BayesianNAM(\n",
    "    cat_feature_info=cat_feature_info,\n",
    "    num_feature_info=num_feature_info,\n",
    "    config=DefaultBayesianNAMConfig(),\n",
    "    subnetwork_config=DefaultBayesianNNConfig()\n",
    ")\n",
    "\n",
    "CV = True\n",
    "if CV:\n",
    "    model.cross_validation(\n",
    "        num_features=num_feature_inputs,\n",
    "        cat_features=cat_feature_inputs,\n",
    "        target=jnp.array(y),\n",
    "    )\n",
    "else:\n",
    "    model.train_model(\n",
    "        num_features={\n",
    "            feature_name: jnp.array(\n",
    "                num_features.iloc[:, col_idx]\n",
    "            ) for col_idx, feature_name in enumerate(num_features.columns)\n",
    "        },\n",
    "        cat_features={\n",
    "            feature_name: jnp.array(\n",
    "                cat_features.iloc[:, col_idx]\n",
    "            ) for col_idx, feature_name in enumerate(cat_features.columns)\n",
    "        },\n",
    "        target=jnp.array(y),\n",
    "    )"
   ],
   "id": "32b613fda0919335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:39:52,446 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,446 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,462 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,463 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,463 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,463 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,463 - namgcv.basemodels.bnn - INFO - Bayesian NN successfully initialized.\n",
      "2025-03-13 18:39:52,596 - namgcv.basemodels.bnam - INFO - \n",
      "+---------------------------------------+\n",
      "| Bayesian NAM successfully initialized.|\n",
      "+---------------------------------------+\n",
      "\n",
      "2025-03-13 18:39:52,601 - namgcv.basemodels.bnam - INFO - Numerical feature network: displacement\n",
      "Network architecture:\n",
      "Layer 0: Linear(1 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,601 - namgcv.basemodels.bnam - INFO - Numerical feature network: horsepower\n",
      "Network architecture:\n",
      "Layer 0: Linear(1 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,601 - namgcv.basemodels.bnam - INFO - Numerical feature network: weight\n",
      "Network architecture:\n",
      "Layer 0: Linear(1 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,601 - namgcv.basemodels.bnam - INFO - Numerical feature network: acceleration\n",
      "Network architecture:\n",
      "Layer 0: Linear(1 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,601 - namgcv.basemodels.bnam - INFO - Categorical feature network: cylinders\n",
      "Network architecture:\n",
      "Layer 0: Linear(5 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,605 - namgcv.basemodels.bnam - INFO - Categorical feature network: model\n",
      "Network architecture:\n",
      "Layer 0: Linear(13 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "2025-03-13 18:39:52,605 - namgcv.basemodels.bnam - INFO - Categorical feature network: origin\n",
      "Network architecture:\n",
      "Layer 0: Linear(3 -> 8) \n",
      "\tLayerNorm \n",
      "\tActivation: relu \n",
      "\tDropout(p=0.1) \n",
      "Layer 1: Linear(8 -> 1) \n",
      "\n",
      "Warning: File cv_0_outer_0_inner_data_loader not found.\n",
      "2025-03-13 18:39:57,666 - namgcv.basemodels.bnam - INFO - Data loader provided: \n",
      " TabularAdditiveModelDataLoader:\n",
      " | Data: cv_0_outer_0_inner_data_loader\n",
      " | Task: regr\n",
      " | TabularAdditiveModelDataLoader\n",
      " | Train: 223\n",
      " | Valid: 75\n",
      " | Test:  100\n",
      "2025-03-13 18:39:57,666 - namgcv.basemodels.bnam - INFO - Deep ensemble initialization enabled. Training deep ensemble...\n",
      "2025-03-13 18:39:57,666 - namgcv.basemodels.bnam - INFO - \n",
      "+--------------------------------------------+\n",
      "| Deterministic NAM successfully initialized.|\n",
      "+--------------------------------------------+\n",
      "\n",
      "2025-03-13 18:40:00,779 - namgcv.basemodels.bnam - INFO - Starting warm-start training for chain(s) [0 1 2 3 4 5 6 7 8 9]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 499 | NLL=(Train:[6.239, 5.069, 5.371, 6.218, 7.175, 7.863, 6.265, 13.326, 5.580, 6.312], Val:[6.239, 5.069, 5.371, 6.218, 7.175, 7.863, 6.265, 13.326, 5.580, 6.312]) | RMSE=(Train:[21.379, 20.788, 20.534, 20.966, 21.235, 21.253, 21.000, 21.511, 20.389, 19.994], Val:[21.379, 20.788, 20.534, 20.966, 21.235, 21.253, 21.000, 21.511, 20.389, 19.994]): 100%|██████████| 500/500 [01:00<00:00,  8.24it/s]                                                          \n",
      "100%|██████████| 500/500 [01:00<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 18:41:03,539 - namgcv.basemodels.bnam - INFO - (Test NLL=[6.587, 6.044, 5.683, 6.210, 7.478, 8.103, 7.018, 10.649, 5.878, 6.319] | Test RMSE=[22.273, 21.742, 21.339, 21.719, 22.079, 22.080, 21.810, 22.194, 21.250, 20.863])\n",
      "2025-03-13 18:41:06,377 - namgcv.basemodels.bnam - INFO - Finished warm-start training for chain 1 of 10.\n",
      "2025-03-13 18:41:06,434 - namgcv.basemodels.bnam - INFO - Deep Ensemble 0 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,442 - namgcv.basemodels.bnam - INFO - Deep Ensemble 1 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,443 - namgcv.basemodels.bnam - INFO - Deep Ensemble 2 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,460 - namgcv.basemodels.bnam - INFO - Deep Ensemble 3 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,460 - namgcv.basemodels.bnam - INFO - Deep Ensemble 4 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,478 - namgcv.basemodels.bnam - INFO - Deep Ensemble 5 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,478 - namgcv.basemodels.bnam - INFO - Deep Ensemble 6 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,505 - namgcv.basemodels.bnam - INFO - Deep Ensemble 7 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,518 - namgcv.basemodels.bnam - INFO - Deep Ensemble 8 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,528 - namgcv.basemodels.bnam - INFO - Deep Ensemble 9 saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:06,594 - namgcv.basemodels.bnam - INFO - Deep Ensemble warm-start metrics saved to C:\\Users\\Aleks Lyubenov\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\..\\bnam_de_warmstart_checkpoints\\warmstart\n",
      "2025-03-13 18:41:08,796 - namgcv.basemodels.bnam - INFO - Starting sampling process for chain(s) [0 1 2 3 4 5 6 7 8 9]...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cf26b4e386841c58f91b2443d30636a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5aad081210ed4dc7bba0dc8a22959563"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43a54176f48044eb82b20501763e2531"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0490660aefd34096889b1f905ab9812d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "539a5d520313447b97ad8a109ca18c18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c99905243dec46b5a49e65c76c624863"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fecf6f33fd549b9b2a6cda51258a45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91ce49cca25043d09ac7065fbf0127b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69dd7fe3944c4a088b37b8356cb91bbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85f4fa67719d493c92814510cb944c28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 40\u001B[0m\n\u001B[0;32m     38\u001B[0m CV \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CV:\n\u001B[1;32m---> 40\u001B[0m     model\u001B[38;5;241m.\u001B[39mcross_validation(\n\u001B[0;32m     41\u001B[0m         num_features\u001B[38;5;241m=\u001B[39mnum_feature_inputs,\n\u001B[0;32m     42\u001B[0m         cat_features\u001B[38;5;241m=\u001B[39mcat_feature_inputs,\n\u001B[0;32m     43\u001B[0m         target\u001B[38;5;241m=\u001B[39mjnp\u001B[38;5;241m.\u001B[39marray(y),\n\u001B[0;32m     44\u001B[0m     )\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain_model(\n\u001B[0;32m     47\u001B[0m         num_features\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m     48\u001B[0m             feature_name: jnp\u001B[38;5;241m.\u001B[39marray(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     57\u001B[0m         target\u001B[38;5;241m=\u001B[39mjnp\u001B[38;5;241m.\u001B[39marray(y),\n\u001B[0;32m     58\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\[LMU] Classes\\[5] Winter 2025\\Thesis\\Repos\\NAMgcv\\namgcv\\basemodels\\bnam.py:1309\u001B[0m, in \u001B[0;36mBayesianNAM.cross_validation\u001B[1;34m(self, num_features, cat_features, target)\u001B[0m\n\u001B[0;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.4\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1220\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.4\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1235\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:13:41.663834500Z",
     "start_time": "2025-03-13T11:34:21.413446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_feature_contributions(\n",
    "        num_features: dict,\n",
    "        cat_features: dict,\n",
    "        interaction_features: dict,\n",
    "        submodel_contributions: dict,\n",
    "        num_outputs: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots feature contributions for numerical, categorical, and interaction features.\n",
    "    Now includes a separate heatmap panel for the uncertainty of interaction features.\n",
    "    Args:\n",
    "        num_features (Dict[str, jnp.ndarray]):\n",
    "            Dictionary of numerical features.\n",
    "        cat_features (Dict[str, jnp.ndarray]):\n",
    "            Dictionary of categorical features.\n",
    "        interaction_features (Dict[str, jnp.ndarray]):\n",
    "            Dictionary of interaction features.\n",
    "        submodel_contributions (Dict[str, np.ndarray]):\n",
    "            Dictionary of feature contributions with keys as feature names and values\n",
    "            as numpy arrays of shape [num_samples, batch_size].\n",
    "        target (jnp.ndarray):\n",
    "            Target variable.\n",
    "        aleatoric_uncertainty (Dict[str, jnp.ndarray]):\n",
    "            Dictionary of aleatoric uncertainty estimates for each feature.\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set_style(\"whitegrid\", {\"axes.facecolor\": \".9\"})\n",
    "    # Plot numerical features\n",
    "    for feature_dict in [num_features, cat_features, interaction_features]:\n",
    "        if not feature_dict:\n",
    "            continue\n",
    "\n",
    "        num_plots = len(feature_dict)\n",
    "        fig, ax = plt.subplots(\n",
    "            nrows=num_plots, ncols=num_outputs,\n",
    "            figsize=(12*num_outputs, 6 * num_plots),\n",
    "            squeeze=False\n",
    "        )\n",
    "        for i, (feature_name, feature_array) in enumerate(feature_dict.items()):\n",
    "            feature_values = np.array(feature_array).flatten()  # Convert JAX array to NumPy\n",
    "\n",
    "            # Shape: [num_mcmc_samples, batch_size, network_output_dim]\n",
    "            contributions = submodel_contributions[feature_name]\n",
    "\n",
    "            # [batch_size, network_output_dim]\n",
    "            mean_contribution_all_params = contributions.mean(axis=0)\n",
    "            for j in range(num_outputs):\n",
    "                mean_param_contribution = mean_contribution_all_params[:, j] if num_outputs > 1 else mean_contribution_all_params\n",
    "\n",
    "                # Create vertical colored bars with color intensity based on density\n",
    "                sorted_idx = np.argsort(feature_values)\n",
    "                feature_values_sorted = feature_values[sorted_idx]\n",
    "                mean_param_contribution_sorted = mean_param_contribution[sorted_idx]\n",
    "\n",
    "                ax_to_plot = ax[i, j]\n",
    "                # Plot the centered partial contributions.\n",
    "                sns.lineplot(\n",
    "                    x=feature_values_sorted,\n",
    "                    y=mean_param_contribution_sorted,\n",
    "                    color=GREEN_RGB_COLORS[0],\n",
    "                    label=\"Mean Output Parameter Contribution\",\n",
    "                    ax=ax_to_plot,\n",
    "                    drawstyle=\"steps\" if np.all(np.isin(feature_values, [0, 1])) else None\n",
    "                )\n",
    "\n",
    "                uncertainty = np.std(\n",
    "                    submodel_contributions[feature_name][:, :, j],\n",
    "                    axis=0\n",
    "                )[sorted_idx] \\\n",
    "                    if num_outputs > 1 \\\n",
    "                    else np.std(\n",
    "                        submodel_contributions[feature_name],\n",
    "                        axis=0\n",
    "                    )[sorted_idx]\n",
    "                ax_to_plot.fill_between(\n",
    "                    feature_values_sorted,\n",
    "                    mean_param_contribution_sorted - 1.96 * uncertainty,\n",
    "                    mean_param_contribution_sorted + 1.96 * uncertainty,\n",
    "                    alpha=0.3,\n",
    "                    color=GREEN_RGB_COLORS[-5],\n",
    "                    label=\"Epistemic Uncertainty - 95% Interval\"\n",
    "                )\n",
    "\n",
    "                num_bins = 30\n",
    "                counts, bin_edges = np.histogram(feature_values, bins=num_bins)\n",
    "                norm_counts = counts / counts.max()\n",
    "                fixed_height = ax_to_plot.get_ylim()[1] - ax_to_plot.get_ylim()[0]\n",
    "                for k in range(num_bins):\n",
    "                    ax_to_plot.bar(\n",
    "                        bin_edges[k],\n",
    "                        height=fixed_height,\n",
    "                        bottom=ax_to_plot.get_ylim()[0],\n",
    "                        width=bin_edges[k + 1] - bin_edges[k],\n",
    "                        color=plt.cm.Blues(norm_counts[k]),\n",
    "                        alpha=0.3\n",
    "                    )\n",
    "\n",
    "                ax_to_plot.set_xlabel(f\"{feature_name}\", fontsize=12)\n",
    "                ax_to_plot.set_ylabel(\"Feature Contribution\", fontsize=12)\n",
    "                ax_to_plot.set_title(f\"Feature Contribution for {feature_name}\", fontsize=12)\n",
    "                ax_to_plot.legend(loc='best', fontsize=12, frameon=False)\n",
    "                ax_to_plot.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "61523b45d2f5bd1b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:29:11.847285Z",
     "start_time": "2025-03-13T17:29:11.826067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_calibration(\n",
    "        pred_samples,\n",
    "        y_test\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will generate a plot, which displays the true frequency of points in each confidence interval relative to the predicted fraction of points in that interval.\n",
    "    The closer the points are to the diagonal, the better the calibration.\n",
    "\n",
    "    Formally, we choose m confidence levels 0<=p_1<=...<=p_m<=1. Then, for each threshold p_j, we compute the empirical frequency p_j_hat.\n",
    "\n",
    "    calibration = sum_{j=1}^m (p_j_hat - p_j)**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    from properscoring import crps_ensemble  # Install via: pip install properscoring\n",
    "\n",
    "    # --- Assumptions ---\n",
    "    # model: your Bayesian NAM model that outputs predictive samples in regression\n",
    "    # X_test: test input data\n",
    "    # y_test: true target values (continuous)\n",
    "    # --- End assumptions ---\n",
    "\n",
    "\n",
    "    lower_quantile = 5  # 5th percentile\n",
    "    upper_quantile = 95 # 95th percentile\n",
    "\n",
    "    lower_bounds = np.percentile(pred_samples, lower_quantile, axis=0)\n",
    "    upper_bounds = np.percentile(pred_samples, upper_quantile, axis=0)\n",
    "\n",
    "    # Calculate the empirical coverage: fraction of true targets within the prediction intervals\n",
    "    coverage = np.mean((y_test >= lower_bounds) & (y_test <= upper_bounds))\n",
    "    print(f\"Empirical coverage of 90% prediction interval: {coverage:.3f}\")\n",
    "\n",
    "    # Plot a subset of predictions with their uncertainty intervals\n",
    "    n_plot = min(50, len(y_test))  # Plot at most 50 examples for clarity\n",
    "    indices = np.random.choice(len(y_test), n_plot, replace=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(range(n_plot),\n",
    "                 np.median(pred_samples[:, indices], axis=0),\n",
    "                 yerr=[np.median(pred_samples[:, indices], axis=0) - lower_bounds[indices],\n",
    "                       upper_bounds[indices] - np.median(pred_samples[:, indices], axis=0)],\n",
    "                 fmt='o', capsize=5, label='Prediction median with 90% CI')\n",
    "    plt.scatter(range(n_plot), y_test[indices], color='red', label='True value')\n",
    "    plt.xlabel(\"Test sample index\")\n",
    "    plt.ylabel(\"Target value\")\n",
    "    plt.title(\"Prediction Intervals vs. True Values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Optional: Compute the Continuous Ranked Probability Score (CRPS) ---\n",
    "    # CRPS is a proper scoring rule for probabilistic forecasts.\n",
    "    # It measures the difference between the predicted distribution and the true outcome.\n",
    "    # Lower CRPS values indicate better probabilistic calibration.\n",
    "    crps = np.mean([crps_ensemble(y_test[i], pred_samples[:, i]) for i in range(len(y_test))])\n",
    "    print(f\"Mean CRPS: {crps:.3f}\")\n"
   ],
   "id": "1da0358d3e1a8dda",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:13:41.685983900Z",
     "start_time": "2025-03-13T11:34:25.680039Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianNAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpyro\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minfer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Predictive\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logsumexp\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_negative_log_likelihood\u001B[39m(\n\u001B[1;32m----> 7\u001B[0m     model: BayesianNAM,\n\u001B[0;32m      8\u001B[0m     split: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m     num_pp_samples: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     10\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m     11\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m    Compute the average negative log likelihood (NLL) of a dataset (train, val or test)\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m    using the model’s posterior predictive distribution.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;124;03m        expected dimensions.\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# Ensure that posterior samples are available.\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'BayesianNAM' is not defined"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "import jax.numpy as jnp\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import Predictive\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def compute_negative_log_likelihood(\n",
    "    model: BayesianNAM,\n",
    "    split: str = \"test\",\n",
    "    num_pp_samples: int = 100,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the average negative log likelihood (NLL) of a dataset (train, val or test)\n",
    "    using the model’s posterior predictive distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BayesianNAM\n",
    "        The Bayesian Neural Additive Model instance. It must have been trained so that\n",
    "        model.posterior_samples is available and model.data_loader is initialized.\n",
    "    split : str, optional\n",
    "        Which split of the data to evaluate (e.g., \"train\", \"valid\", or \"test\"). Default is \"test\".\n",
    "    num_pp_samples : int, optional\n",
    "        Number of predictive samples to draw from the posterior predictive distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average negative log likelihood over the dataset.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If posterior samples are not found or if the predicted parameters do not have the\n",
    "        expected dimensions.\n",
    "    \"\"\"\n",
    "    # Ensure that posterior samples are available.\n",
    "    if model.posterior_samples is None:\n",
    "        raise ValueError(\"Posterior samples not found. Please run MCMC sampling first.\")\n",
    "\n",
    "    # Set up the Predictive distribution using the model's MCMC posterior samples.\n",
    "    predictive = Predictive(\n",
    "        model.model,\n",
    "        posterior_samples=model.posterior_samples,\n",
    "        num_samples=num_pp_samples\n",
    "    )\n",
    "\n",
    "    # Get the data for the specified split (assumed to be full-batch).\n",
    "    batch_iter = model.data_loader.iter(split=split, batch_size=None)\n",
    "    data_batch = next(batch_iter)\n",
    "    y_obs = data_batch[\"target\"]  # shape: [batch_size, ...]\n",
    "\n",
    "    # Generate posterior predictive samples.\n",
    "    # (Note: The model.model function is written to accept the data_loader from the model.)\n",
    "    preds = predictive(model._single_rng_key, data_loader=model.data_loader, is_training=False)\n",
    "\n",
    "    # Extract the final distributional parameters.\n",
    "    # For non-mixture models, we expect at least two parameters: (mu, sigma).\n",
    "    # For mixture models, the model outputs parameters for each mixture component and separate mixture coefficients.\n",
    "    final_params = preds[\"final_params\"]  # shape: [num_pp_samples, batch_size, D]\n",
    "\n",
    "    # Check if the model uses a mixture (num_mixture_components > 1).\n",
    "    if model.config.num_mixture_components > 1:\n",
    "        # Assume D is 2*k where k is the number of mixture components.\n",
    "        k = model.config.num_mixture_components\n",
    "        if final_params.shape[-1] < 2 * k:\n",
    "            raise ValueError(\"Final parameters do not contain enough dimensions for a mixture model.\")\n",
    "        # Extract means and standard deviations for each mixture component.\n",
    "        mu = final_params[..., :k]    # shape: [num_pp_samples, batch_size, k]\n",
    "        sigma = final_params[..., k:2*k]  # shape: [num_pp_samples, batch_size, k]\n",
    "        # Extract and normalize the mixture coefficients.\n",
    "        mixture_coeff = preds[\"final_mixture_coefficients\"]  # shape: [num_pp_samples, batch_size, k]\n",
    "        # Construct a mixture of Normal distributions.\n",
    "        comp_dist = dist.Normal(loc=mu, scale=sigma)\n",
    "        mixture_dist = dist.MixtureSameFamily(\n",
    "            mixing_distribution=dist.Categorical(probs=mixture_coeff),\n",
    "            component_distribution=comp_dist\n",
    "        )\n",
    "        # Compute the log probability for each predictive sample.\n",
    "        log_probs = mixture_dist.log_prob(y_obs)  # shape: [num_pp_samples, batch_size]\n",
    "    else:\n",
    "        # For the single-component case, expect at least two dimensions in final_params.\n",
    "        if final_params.shape[-1] < 2:\n",
    "            raise ValueError(\"Final parameters must include sigma for computing the likelihood.\")\n",
    "        mu = final_params[..., 0]      # shape: [num_pp_samples, batch_size]\n",
    "        sigma = final_params[..., 1]   # shape: [num_pp_samples, batch_size]\n",
    "        # Define the Normal distribution.\n",
    "        single_dist = dist.Normal(loc=mu, scale=sigma)\n",
    "        log_probs = single_dist.log_prob(y_obs)  # shape: [num_pp_samples, batch_size]\n",
    "\n",
    "    # Average the log probabilities over the predictive samples using log-sum-exp.\n",
    "    # This gives an estimate of the log predictive density for each data point.\n",
    "    log_prob_mean = logsumexp(log_probs, axis=0) - jnp.log(num_pp_samples)  # shape: [batch_size]\n",
    "\n",
    "    # Compute the negative log likelihood (average over data points).\n",
    "    nll = -jnp.mean(log_prob_mean)\n",
    "\n",
    "    return float(nll)"
   ],
   "id": "164cfaaa3bc5b93c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for model in all_runs_models:\n",
    "    final_params, _, submodel_contributions = model.predict()\n",
    "\n",
    "    batch_iter = model.data_loader.iter(\n",
    "        \"test\",\n",
    "        batch_size=None\n",
    "    )\n",
    "    data_dict = next(batch_iter)  # First and only batch.\n",
    "    num_features = data_dict[\"feature\"][\"numerical\"]\n",
    "    cat_features = data_dict[\"feature\"][\"categorical\"]\n",
    "    target = data_dict[\"target\"]\n",
    "\n",
    "    interaction_feature_information = {}\n",
    "    all_features = {**num_features, **cat_features}\n",
    "    for interaction_name in submodel_contributions.keys():\n",
    "        if \":\" not in interaction_name:\n",
    "            continue\n",
    "\n",
    "        feature_names = interaction_name.split(\":\")\n",
    "        interaction_feature_information[interaction_name] = jnp.concatenate(\n",
    "            [jnp.expand_dims(all_features[name], axis=-1) for name in feature_names],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "    plot_feature_contributions(\n",
    "            num_features=num_features,\n",
    "            cat_features=cat_features,\n",
    "            interaction_features=interaction_feature_information,\n",
    "            submodel_contributions=submodel_contributions,\n",
    "            num_outputs=1\n",
    "    )\n",
    "\n",
    "    # Compute the average negative log likelihood (NLL) of the test set.\n",
    "    nll = compute_negative_log_likelihood(\n",
    "        model=model,\n",
    "        split=\"test\",\n",
    "        num_pp_samples=100\n",
    "    )\n",
    "    print(nll)\n",
    "\n",
    "    evaluate_calibration(pred_samples=jnp.squeeze(final_params), y_test=target)\n"
   ],
   "id": "213d240407ab110b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:13:41.696178Z",
     "start_time": "2025-03-13T11:06:20.368865Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f49259832c93cc3f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:13:41.696178Z",
     "start_time": "2025-03-13T11:08:50.068749Z"
    }
   },
   "cell_type": "code",
   "source": "final_params.shape",
   "id": "f7aca30018188b1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 40, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:13:41.701949400Z",
     "start_time": "2025-03-13T11:07:42.087165Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9072699e4b4da422",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5ca590dd93bb97f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
